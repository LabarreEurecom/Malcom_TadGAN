{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red\" align=\"center\">TadGAN - Time series anomaly detection using GAN</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import needed/required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Model's Architecture:</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](./images/Model's_Architecture-Diagram.PNG) ![image-2.png](./images/Model's_Architecture-Description.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">TadGan Pseudo-Code:</h1> \n",
    "<h4 align=\"center\">(from the paper)</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](./images/TadGan-Algo.PNG) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X is an univariate series with xi scalars. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Define the Model</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder_path, signal_shape=100):\n",
    "        '''\n",
    "        encoder_path: ?? (idk its usefulness)\n",
    "        signal_shape = sequence length of the time serie\n",
    "        '''\n",
    "        super(Encoder, self).__init__()\n",
    "        self.signal_shape = signal_shape\n",
    "        \n",
    "        # lstm = long short-term memory\n",
    "        self.lstm = nn.LSTM(input_size=self.signal_shape, \n",
    "                            hidden_size=20, # Latent space dimension\n",
    "                            num_layers=1,   # Nb of layers\n",
    "                            bidirectional=True) # Forward & Backward\n",
    "        \n",
    "        self.dense = nn.Linear(in_features=40, out_features=20)\n",
    "        self.encoder_path = encoder_path\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(1, 64, self.signal_shape).float()\n",
    "        print(\"x.view\",x)\n",
    "        x, (hn, cn) = self.lstm(x)\n",
    "        print(self.lstm(x, (hn,cn)))\n",
    "        x = self.dense(x)\n",
    "        return (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, decoder_path, signal_shape=100):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.signal_shape = signal_shape\n",
    "        self.lstm = nn.LSTM(input_size=20, hidden_size=64, num_layers=2, bidirectional=True)\n",
    "        self.dense = nn.Linear(in_features=128, out_features=self.signal_shape)\n",
    "        self.decoder_path = decoder_path\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, (hn, cn) = self.lstm(x)\n",
    "        x = self.dense(x)\n",
    "        return (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THE CRITICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code we introduce 2  two adversarial Critics (aka discriminators) Cx and Cz. The goal of Cx is to distinguish between the real time series sequences from X and the generated time series sequences from G(z), while Cz\n",
    "measures the performance of the mapping into latent space. \n",
    "\n",
    "In other words, G is trying to fool Cx by generating real-looking sequences. \n",
    " \n",
    "Thus, our high-level objective consists of two terms: \n",
    "    (1) Wasserstein losses, to match the distribution of generated time series sequences to the data distribution in\n",
    "the target domain\n",
    "    (2) cycle consistency losses, to prevent the contradiction between E and G (encoder and decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the CriticX :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REMINDER: The goal of Cx is to distinguish between the real time series sequences from X and the generated time series sequences from G(z) by using wassertein loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](./images/Wasserstein-Z-X-Cx.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Look more in detailed how it is defined in term of coding\n",
    "class CriticX(nn.Module):\n",
    "    def __init__(self, critic_x_path, signal_shape=100):\n",
    "        super(CriticX, self).__init__()\n",
    "        self.signal_shape = signal_shape\n",
    "        self.dense1 = nn.Linear(in_features=self.signal_shape, out_features=20)\n",
    "        self.dense2 = nn.Linear(in_features=20, out_features=1)\n",
    "        self.critic_x_path = critic_x_path\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(1, 64, self.signal_shape).float()\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the CriticZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cz measures the performance of the mapping into latent space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](./images/Wasserstein-X-Z-Cz.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticZ(nn.Module):\n",
    "    def __init__(self, critic_z_path):\n",
    "        super(CriticZ, self).__init__()\n",
    "        self.dense1 = nn.Linear(in_features=20, out_features=1)\n",
    "        self.critic_z_path = critic_z_path\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense1(x)\n",
    "        return (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](./images/Cycle_Consistency_Loss.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](./images/Full_objective.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To reconstruct the signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](./images/Signal_reconstruction.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define unroll_signal function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unroll_signal(self, x):\n",
    "    x = np.array(x).reshape(100)\n",
    "    return np.median(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/launch_training.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(self):\n",
    "    \"\"\"\n",
    "    Returns a dataframe with original value, reconstructed value, reconstruction error, critic score\n",
    "    \"\"\"\n",
    "    df = self.test_dataset.copy()\n",
    "    X_ = list()\n",
    "\n",
    "    RE = list()  #Reconstruction error\n",
    "    CS = list()  #Critic score\n",
    "\n",
    "    for i in range(0, df.shape[0]):\n",
    "        x = df.rolled_signal[i]\n",
    "        x = tf.reshape(x, (1, 100, 1))\n",
    "        z = encoder(x) #E(x)\n",
    "        z = tf.expand_dims(z, axis=2)\n",
    "        x_ = decoder(z) #G(E(x))\n",
    "\n",
    "        re = dtw_reconstruction_error(tf.squeeze(x_).numpy(), tf.squeeze(x).numpy()) #reconstruction error\n",
    "        cs = critic_x(x)\n",
    "        cs = tf.squeeze(cs).numpy()\n",
    "        RE.append(re)\n",
    "        CS.append(cs)\n",
    "\n",
    "        x_ = unroll_signal(x_)\n",
    "\n",
    "        X_.append(x_)\n",
    "\n",
    "    df['generated_signals'] = X_\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Define the Anomaly Detection</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, encoder, decoder, critic_x):\n",
    "    reconstruction_error = list()\n",
    "    critic_score = list()\n",
    "    y_true = list()\n",
    "\n",
    "    for batch, sample in enumerate(test_loader):\n",
    "        reconstructed_signal = decoder(encoder(sample['signal']))\n",
    "        reconstructed_signal = torch.squeeze(reconstructed_signal)\n",
    "\n",
    "        for i in range(0, 64):\n",
    "            x_ = reconstructed_signal[i].detach().numpy()\n",
    "            x = sample['signal'][i].numpy()\n",
    "            y_true.append(int(sample['anomaly'][i].detach()))\n",
    "            reconstruction_error.append(dtw_reconstruction_error(x, x_))\n",
    "        critic_score.extend(torch.squeeze(critic_x(sample['signal'])).detach().numpy())\n",
    "\n",
    "    reconstruction_error = stats.zscore(reconstruction_error)\n",
    "    critic_score = stats.zscore(critic_score)\n",
    "    anomaly_score = reconstruction_error * critic_score\n",
    "    y_predict = detect_anomaly(anomaly_score)\n",
    "    y_predict = prune_false_positive(y_predict, anomaly_score, change_threshold=0.1)\n",
    "    find_scores(y_true, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other error metrics - point wise difference, Area difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtw_reconstruction_error(x, x_):\n",
    "    n, m = x.shape[0], x_.shape[0]\n",
    "    dtw_matrix = np.zeros((n+1, m+1))\n",
    "    for i in range(n+1):\n",
    "        for j in range(m+1):\n",
    "            dtw_matrix[i, j] = np.inf\n",
    "    dtw_matrix[0, 0] = 0\n",
    "\n",
    "    for i in range(1, n+1):\n",
    "        for j in range(1, m+1):\n",
    "            cost = abs(x[i-1] - x_[j-1])\n",
    "            # take last min from a square box\n",
    "            last_min = np.min([dtw_matrix[i-1, j], dtw_matrix[i, j-1], dtw_matrix[i-1, j-1]])\n",
    "            dtw_matrix[i, j] = cost + last_min\n",
    "    return dtw_matrix[n][m]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to prune the false positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_false_positive(is_anomaly, anomaly_score, change_threshold):\n",
    "    #The model might detect a high number of false positives.\n",
    "    #In such a scenario, pruning of the false positive is suggested.\n",
    "    #Method used is as described in the Section 5, part D Identifying Anomalous\n",
    "    #Sequence, sub-part - Mitigating False positives\n",
    "    #TODO code optimization\n",
    "    seq_details = []\n",
    "    delete_sequence = 0\n",
    "    start_position = 0\n",
    "    end_position = 0\n",
    "    max_seq_element = anomaly_score[0]\n",
    "    for i in range(1, len(is_anomaly)):\n",
    "        if i+1 == len(is_anomaly):\n",
    "            seq_details.append([start_position, i, max_seq_element, delete_sequence])\n",
    "        elif is_anomaly[i] == 1 and is_anomaly[i+1] == 0:\n",
    "            end_position = i\n",
    "            seq_details.append([start_position, end_position, max_seq_element, delete_sequence])\n",
    "        elif is_anomaly[i] == 1 and is_anomaly[i-1] == 0:\n",
    "            start_position = i\n",
    "            max_seq_element = anomaly_score[i]\n",
    "        if is_anomaly[i] == 1 and is_anomaly[i-1] == 1 and anomaly_score[i] > max_seq_element:\n",
    "            max_seq_element = anomaly_score[i]\n",
    "\n",
    "    max_elements = list()\n",
    "    for i in range(0, len(seq_details)):\n",
    "        max_elements.append(seq_details[i][2])\n",
    "\n",
    "    max_elements.sort(reverse=True)\n",
    "    max_elements = np.array(max_elements)\n",
    "    change_percent = abs(max_elements[1:] - max_elements[:-1]) / max_elements[1:]\n",
    "\n",
    "    #Appending 0 for the 1 st element which is not change percent\n",
    "    delete_seq = np.append(np.array([0]), change_percent < change_threshold)\n",
    "\n",
    "    #Mapping max element and seq details\n",
    "    for i, max_elt in enumerate(max_elements):\n",
    "        for j in range(0, len(seq_details)):\n",
    "            if seq_details[j][2] == max_elt:\n",
    "                seq_details[j][3] = delete_seq[i]\n",
    "\n",
    "    for seq in seq_details:\n",
    "        if seq[3] == 1: #Delete sequence\n",
    "            is_anomaly[seq[0]:seq[1]+1] = [0] * (seq[1] - seq[0] + 1)\n",
    " \n",
    "    return is_anomaly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to detect anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we create the training sample by subdividing the original time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomaly(anomaly_score):\n",
    "    '''\n",
    "    define the training sample by introducing a sliding window \n",
    "    a size t =len(anomaly_score) // 3 \n",
    "    and a stepsize s = len(anomaly_score) // (3 * 10)\n",
    "    \n",
    "    Thus the original time series is divided into N sub-sequences X = {(x{1...t}i)}N{i=1}, where N =(T âˆ’t)/s\n",
    "    '''\n",
    "    window_size = len(anomaly_score) // 3\n",
    "    step_size = len(anomaly_score) // (3 * 10)\n",
    "\n",
    "    is_anomaly = np.zeros(len(anomaly_score))\n",
    "\n",
    "    for i in range(0, len(anomaly_score) - window_size, step_size):\n",
    "        window_elts = anomaly_score[i:i+window_size] # sliding window of size window_size\n",
    "        window_mean = np.mean(window_elts)\n",
    "        window_std = np.std(window_mean)\n",
    "\n",
    "        for j, elt in enumerate(window_elts):\n",
    "            if (window_mean - 3 * window_std) < elt < (window_mean + 3 * window_std):\n",
    "                is_anomaly[i + j] = 0\n",
    "            else:\n",
    "                is_anomaly[i + j] = 1\n",
    "\n",
    "    return is_anomaly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to find score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Reminder</u>:\n",
    "\n",
    "$ \\boxed{ Accuracy = \\frac {T_{P}+T_{N}} {N_{total}} } $.\n",
    "<b style=\"color:red\">ACCURACY used to know HOW WELL the PREDICTION IS</b>.\n",
    "\n",
    "$ \\boxed{ Precision = \\frac {T_{P}} {T_{P}+F_{P}} } $.\n",
    "<b style=\"color:red\">PRECISION used to know HOW WELL/ACCURATE a POSITIVE IS DETERMINED</b>.\n",
    "\n",
    "$ \\boxed{ Recall/Sensitivity = \\frac {T_{P}} {T_{P}+F_{N}} } $.\n",
    "<b style=\"color:red\">RECALL or SENSITIVITY used to know HOW MUCH POSITIVES ARE DETERMINED</b>.\n",
    "\n",
    "$ \\boxed{ F1  score = \\frac {2} {Precision^{-1} + Recall^{-1}} = \\frac {2 . Precision . Recall} {Precision + Recall} = \\frac {T_{P}} {T_{P} + \\frac{1}{2} (F_{N}+F_{P})} } $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here\n",
    "\n",
    "![image.png](./images/TP_FP_FN.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_scores(y_true, y_predict):\n",
    "    tp = tn = fp = fn = 0\n",
    "\n",
    "    for i in range(0, len(y_true)):\n",
    "        if y_true[i] == 1 and y_predict[i] == 1:\n",
    "            tp += 1\n",
    "        elif y_true[i] == 1 and y_predict[i] == 0:\n",
    "            fn += 1\n",
    "        elif y_true[i] == 0 and y_predict[i] == 0:\n",
    "            tn += 1\n",
    "        elif y_true[i] == 0 and y_predict[i] == 1:\n",
    "            fp += 1\n",
    "\n",
    "    print ('Accuracy {:.2f}'.format((tp + tn)/(len(y_true))))\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    print ('Precision {:.2f}'.format(precision))\n",
    "    print ('Recall {:.2f}'.format(recall))\n",
    "    print ('F1 Score {:.2f}'.format(2 * precision * recall / (precision + recall)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Apply the model</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='train.log', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignalDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.signal_df = pd.read_csv(path)\n",
    "        self.signal_columns = self.make_signal_list()\n",
    "        self.make_rolling_signals()\n",
    "\n",
    "    def make_signal_list(self):\n",
    "        signal_list = list()\n",
    "        for i in range(-50, 50):\n",
    "            signal_list.append('signal'+str(i))\n",
    "        return signal_list\n",
    "\n",
    "    def make_rolling_signals(self):\n",
    "        for i in range(-50, 50):\n",
    "            self.signal_df['signal'+str(i)] = self.signal_df['signal'].shift(i)\n",
    "        self.signal_df = self.signal_df.dropna()\n",
    "        self.signal_df = self.signal_df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.signal_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.signal_df.loc[idx]\n",
    "        x = row[self.signal_columns].values.astype(float)\n",
    "        x = torch.from_numpy(x)\n",
    "        return {'signal':x, 'anomaly':row['anomaly']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the CriticX training (compute the wassertein loss for critic X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C KOI GP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from a sample a data (subdivision of the original time series) we\n",
    "generate Z={(z1...ki)}i=1 from a random space following normal distribution, where k denotes the dimension of the\n",
    "latent space. Here we encode x to have z.\n",
    "\n",
    "STEP 1 :\n",
    "resizing x\n",
    "\n",
    "\n",
    "STEP 2 :\n",
    "We Estimate the Anomaly Scores with Critic X Outputs\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{m} \\sum_{i=1}^m C_{x}(x_{i}) \\\\\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "STEP 3 :\n",
    "We Estimate the Anomaly Scores with Critic Z Outputs\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{m} \\sum_{i=1}^m C_{x}(G(z_{i})) \\\\\n",
    "\\end{equation}\n",
    "\n",
    "STEP 4 :\n",
    "\\begin{equation}\n",
    "gp(x_{i}, G(z_{i})) \\\\\n",
    "\\end{equation}\n",
    "\n",
    "STEP 5 :\n",
    "\n",
    "step 2 - step 3 + step 4\n",
    "\n",
    "STEP 6 : \n",
    "\n",
    "compute the the gradient on $w_{G}$ and $w_{E}$\n",
    "\n",
    "\n",
    "STEP 7 :\n",
    "update the weights $w_{C_{x}}$\n",
    "\n",
    "\\begin{equation}\n",
    "w_{C_{x}} =w_{C_{x}} + stepSize* adam(w_{C_{x}}, STEP5)\\\\\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def critic_x_iteration(sample):\n",
    "    optim_cx.zero_grad()\n",
    "    \n",
    "    # STEP 1\n",
    "    x = sample['signal'].view(1, batch_size, signal_shape)\n",
    "    \n",
    "    # STEP 2\n",
    "    valid_x = critic_x(x)\n",
    "    valid_x = torch.squeeze(valid_x)\n",
    "    critic_score_valid_x = torch.mean(torch.ones(valid_x.shape) * valid_x) #Wasserstein Loss\n",
    "    \n",
    "    # STEP 3\n",
    "    #The sampled z are the anomalous points - points deviating from actual distribution of z (obtained through encoding x)\n",
    "    z = torch.empty(1, batch_size, latent_space_dim).uniform_(0, 1)\n",
    "    x_ = decoder(z)\n",
    "    fake_x = critic_x(x_)\n",
    "    fake_x = torch.squeeze(fake_x)\n",
    "    critic_score_fake_x = torch.mean(torch.ones(fake_x.shape) * fake_x)  #Wasserstein Loss\n",
    "    \n",
    "    \n",
    "    # STEP 4\n",
    "    alpha = torch.rand(x.shape)\n",
    "    ix = Variable(alpha * x + (1 - alpha) * x_) #Random Weighted Average\n",
    "    ix.requires_grad_(True)\n",
    "    v_ix = critic_x(ix)\n",
    "    v_ix.mean().backward()\n",
    "    gradients = ix.grad\n",
    "    #Gradient Penalty Loss\n",
    "    gp_loss = torch.sqrt(torch.sum(torch.square(gradients).view(-1)))\n",
    "\n",
    "    \n",
    "    # STEP 5\n",
    "    #Critic has to maximize Cx(Valid X) - Cx(Fake X).\n",
    "    #Maximizing the above is same as minimizing the negative.\n",
    "    wl = critic_score_fake_x - critic_score_valid_x\n",
    "    loss = wl + gp_loss\n",
    "    \n",
    "    # STEP 6\n",
    "    loss.backward()\n",
    "    optim_cx.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the CriticZ training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from a sample a data (subdivision of the original time series) we\n",
    "generate Z={(z1...ki)}i=1 from a random space following normal distribution, where k denotes the dimension of the\n",
    "latent space. Here we encode x to have z.\n",
    "\n",
    "STEP 1 :\n",
    "resizing x\n",
    "\n",
    "\n",
    "STEP 2 :\n",
    "We Estimate the Anomaly Scores with Critic X Outputs\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{m} \\sum_{i=1}^m C_{z}(z_{i}) \\\\\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "STEP 3 :\n",
    "We Estimate the Anomaly Scores with Critic Z Outputs\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{m} \\sum_{i=1}^m C_{z}(E(x_{i})) \\\\\n",
    "\\end{equation}\n",
    "\n",
    "STEP 4 :\n",
    "\\begin{equation}\n",
    "gp(z_{i}, E(x_{i})) \\\\\n",
    "\\end{equation}\n",
    "\n",
    "STEP 5 :\n",
    "\n",
    "step 2 - step 3 + step 4\n",
    "\n",
    "STEP 6 : \n",
    "\n",
    "compute the the gradient on $w_{G}$ and $w_{E}$\n",
    "\n",
    "\n",
    "STEP 7 :\n",
    "update the weights $w_{C_{x}}$\n",
    "\n",
    "\\begin{equation}\n",
    "w_{C_{z}} =w_{C_{z}} + stepSize* adam(w_{C_{z}}, STEP5)\\\\\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def critic_z_iteration(sample):\n",
    "    optim_cz.zero_grad()\n",
    "    \n",
    "    # STEP 1\n",
    "    x = sample['signal'].view(1, batch_size, signal_shape)\n",
    "    \n",
    "    # STEP 2\n",
    "    z = encoder(x)\n",
    "    valid_z = critic_z(z)\n",
    "    valid_z = torch.squeeze(valid_z)\n",
    "    critic_score_valid_z = torch.mean(torch.ones(valid_z.shape) * valid_z)\n",
    "\n",
    "    # STEP 3\n",
    "    z_ = torch.empty(1, batch_size, latent_space_dim).uniform_(0, 1)\n",
    "    fake_z = critic_z(z_)\n",
    "    fake_z = torch.squeeze(fake_z)\n",
    "    critic_score_fake_z = torch.mean(torch.ones(fake_z.shape) * fake_z) #Wasserstein Loss\n",
    "\n",
    "    # STEP 4\n",
    "    alpha = torch.rand(z.shape)\n",
    "    iz = Variable(alpha * z + (1 - alpha) * z_) #Random Weighted Average\n",
    "    iz.requires_grad_(True)\n",
    "    v_iz = critic_z(iz)\n",
    "    v_iz.mean().backward()\n",
    "    gradients = iz.grad\n",
    "    gp_loss = torch.sqrt(torch.sum(torch.square(gradients).view(-1)))\n",
    "    \n",
    "    # STEP 5\n",
    "    wl = critic_score_fake_z - critic_score_valid_z\n",
    "    loss = wl + gp_loss\n",
    "    \n",
    "    # STEP 6\n",
    "    loss.backward()\n",
    "    \n",
    "    # STEP 7\n",
    "    optim_cz.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Encoder training(compute the adapted cycle consistency loss on encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADD INFORMATION ON CYCLE CONSISTENCY LOSS AND HOW THE CRITICS OUTPUT ARE USED TO COMPUTE SCORE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from a sample a data (subdivision of the original time series) we\n",
    "generate Z={(z1...ki)}i=1 from a random space following normal distribution, where k denotes the dimension of the\n",
    "latent space. Here we encode x to have z.\n",
    "\n",
    "STEP 1 :\n",
    "resizing x\n",
    "\n",
    "\n",
    "STEP 2 :\n",
    "We Estimate the Anomaly Scores with Critic X Outputs\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{m} \\sum_{i=1}^m C_{x}(x_{i}) \\\\\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "STEP 3 :\n",
    "We Estimate the Anomaly Scores with Critic Z Outputs\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{m} \\sum_{i=1}^m C_{x}(G(z_{i})) \\\\\n",
    "\\end{equation}\n",
    "\n",
    "STEP 4 :\n",
    "\\begin{equation}\n",
    "\\frac{1}{m} \\sum_{i=1}^m ||x_{i} - G(E(x_{i}))||^2 \\\\\n",
    "\\end{equation}\n",
    "\n",
    "STEP 5 :\n",
    "\n",
    "step 2 - step 3 + step 4\n",
    "\n",
    "STEP 6 : \n",
    "\n",
    "compute the the gradient on $w_{G}$ and $w_{E}$\n",
    "\n",
    "\n",
    "STEP 7 :\n",
    "update the weights $w_{G,E}$\n",
    "\n",
    "\\begin{equation}\n",
    "w_{G,E} =w_{G,E} + stepSize* adam(w_{G,E}, STEP5)\\\\\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_iteration(sample):\n",
    "    optim_enc.zero_grad()\n",
    "    \n",
    "    # STEP 1 :\n",
    "    x = sample['signal'].view(1, batch_size, signal_shape)\n",
    "    \n",
    "    # STEP 2\n",
    "    valid_x = critic_x(x)\n",
    "    valid_x = torch.squeeze(valid_x)\n",
    "    critic_score_valid_x = torch.mean(torch.ones(valid_x.shape) * valid_x) #Wasserstein Loss\n",
    "    \n",
    "    # STEP3\n",
    "    z = torch.empty(1, batch_size, latent_space_dim).uniform_(0, 1)\n",
    "    x_ = decoder(z)\n",
    "    fake_x = critic_x(x_)\n",
    "    fake_x = torch.squeeze(fake_x)\n",
    "    critic_score_fake_x = torch.mean(torch.ones(fake_x.shape) * fake_x)\n",
    "    \n",
    "    #STEP4\n",
    "    enc_z = encoder(x)\n",
    "    gen_x = decoder(enc_z)\n",
    "\n",
    "    mse = mse_loss(x.float(), gen_x.float())\n",
    "    loss_enc = mse + critic_score_valid_x - critic_score_fake_x\n",
    "    loss_enc.backward(retain_graph=True)\n",
    "    optim_enc.step()\n",
    "\n",
    "    return loss_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Decoder training (compute the adapted cycle consistency loss on decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 1 :\n",
    "from a sample a data (subdivision of the original time series) we\n",
    "generate Z={(z1...ki)}i=1 from a random space following normal distribution, where k denotes the dimension of the\n",
    "latent space. Here we encode x to have z.\n",
    "\n",
    "\n",
    "STEP 2 :\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{m} \\sum_{i=1}^m C_{z}(z_{i}) \\\\\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "STEP 3 :\n",
    "\\begin{equation}\n",
    "\\frac{1}{m} \\sum_{i=1}^m C_{z}(E(x_{i})) \\\\\n",
    "\\end{equation}\n",
    "\n",
    "STEP 4 :\n",
    "\\begin{equation}\n",
    "\\frac{1}{m} \\sum_{i=1}^m ||x_{i} - G(E(x_{i}))||^2 \\\\\n",
    "\\end{equation}\n",
    "\n",
    "STEP 5 :\n",
    "\n",
    "step 2 - step 3 + step 4\n",
    "\n",
    "STEP 6 : \n",
    "\n",
    "compute the the gradient on $w_{G}$ and $w_{E}$\n",
    "\n",
    "\n",
    "STEP 7 :\n",
    "update the weights $w_{G,E}$\n",
    "\n",
    "\\begin{equation}\n",
    "w_{G,E} =w_{G,E} + stepSize* adam(w_{G,E}, STEP5)\\\\\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_iteration(sample):\n",
    "    optim_dec.zero_grad()# VERY IMPORTANT TO PUT IT HERE\n",
    "    \n",
    "    #STEP 1\n",
    "    x = sample['signal'].view(1, batch_size, signal_shape)\n",
    "    z = encoder(x)\n",
    "    \n",
    "    #STEP 2\n",
    "    #Estimating Anomaly Scores with Critic Z Outputs\n",
    "    valid_z = critic_z(z) # Cz(z)\n",
    "    valid_z = torch.squeeze(valid_z) # resize by making it a univariate variable\n",
    "    critic_score_valid_z = torch.mean(torch.ones(valid_z.shape) * valid_z) #compute the score of criticZ\n",
    "    \n",
    "    #STEP 3\n",
    "    #we use \n",
    "    z_ = torch.empty(1, batch_size, latent_space_dim).uniform_(0, 1)\n",
    "    fake_z = critic_z(z_)\n",
    "    fake_z = torch.squeeze(fake_z)\n",
    "    critic_score_fake_z = torch.mean(torch.ones(fake_z.shape) * fake_z) #\n",
    "    \n",
    "    #STEP 4\n",
    "    enc_z = encoder(x)\n",
    "    gen_x = decoder(enc_z)\n",
    "    mse = mse_loss(x.float(), gen_x.float())\n",
    "    \n",
    "    #STEP 5\n",
    "    loss_dec = mse + critic_score_valid_z - critic_score_fake_z\n",
    "    \n",
    "    #STEP 6\n",
    "    loss_dec.backward(retain_graph=True)\n",
    "    \n",
    "    #STEP 7\n",
    "    optim_dec.step()\n",
    "\n",
    "    return loss_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"./images/tadgan_algo_annotaded.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "n_critics = number of iterations of the critic per\n",
    "n_epoch = number of iterations over the data\n",
    "\n",
    "'''\n",
    "\n",
    "def train(n_epochs=2000):\n",
    "    logging.debug('Starting training')\n",
    "    cx_epoch_loss = list()\n",
    "    cz_epoch_loss = list()\n",
    "    encoder_epoch_loss = list()\n",
    "    decoder_epoch_loss = list()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        logging.debug('Epoch {}'.format(epoch))\n",
    "        n_critics = 5\n",
    "\n",
    "        cx_nc_loss = list()\n",
    "        cz_nc_loss = list()\n",
    "\n",
    "        for i in range(n_critics):\n",
    "            cx_loss = list() \n",
    "            cz_loss = list()\n",
    "            \n",
    "            '''this part corresponds to part 1 of the image above'''\n",
    "            for batch, sample in enumerate(train_loader):\n",
    "                loss = critic_x_iteration(sample) #compute the Wasserstein loss for critic X\n",
    "                cx_loss.append(loss)\n",
    "\n",
    "                loss = critic_z_iteration(sample)#compute the Wasserstein loss for critic Z\n",
    "                cz_loss.append(loss)\n",
    "\n",
    "            cx_nc_loss.append(torch.mean(torch.tensor(cx_loss))) # compute the mean of criticx loss on all samples\n",
    "            cz_nc_loss.append(torch.mean(torch.tensor(cz_loss))) #  compute the mean of criticx loss on all samples\n",
    "\n",
    "        logging.debug('Critic training done in epoch {}'.format(epoch))\n",
    "        encoder_loss = list()\n",
    "        decoder_loss = list()\n",
    "        \n",
    "        '''this part corresponds to part 2 of the image above'''\n",
    "        '''train the generative network E and G with the adapted cycle\n",
    "        consistency loss by minimizing the L2 norm of the difference\n",
    "        between the original and the reconstructed samples'''\n",
    "        for batch, sample in enumerate(train_loader):\n",
    "            enc_loss = encoder_iteration(sample) #compute the adapted cycle consistency loss by minimizing the L2 norm on the encoder\n",
    "            dec_loss = decoder_iteration(sample) #compute the adapted cycle consistency loss by minimizing the L2 norm on the decoder\n",
    "            encoder_loss.append(enc_loss) #add the encoder loss in a tab where the indice corresponds to the sample\n",
    "            decoder_loss.append(dec_loss)\n",
    "        \n",
    "        \n",
    "        # preprocess the data for printing\n",
    "        cx_epoch_loss.append(torch.mean(torch.tensor(cx_nc_loss))) \n",
    "        cz_epoch_loss.append(torch.mean(torch.tensor(cz_nc_loss)))\n",
    "        encoder_epoch_loss.append(torch.mean(torch.tensor(encoder_loss)))\n",
    "        decoder_epoch_loss.append(torch.mean(torch.tensor(decoder_loss)))\n",
    "        \n",
    "        \n",
    "        logging.debug('Encoder decoder training done in epoch {}'.format(epoch))\n",
    "        logging.debug('critic x loss {:.3f} critic z loss {:.3f} \\nencoder loss {:.3f} decoder loss {:.3f}\\n'.format(cx_epoch_loss[-1], cz_epoch_loss[-1], encoder_epoch_loss[-1], decoder_epoch_loss[-1]))\n",
    "        \n",
    "        # save data if the encoder, decoder, critic_x and critic_z every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            torch.save(encoder.state_dict(), encoder.encoder_path)\n",
    "            torch.save(decoder.state_dict(), decoder.decoder_path)\n",
    "            torch.save(critic_x.state_dict(), critic_x.critic_x_path)\n",
    "            torch.save(critic_z.state_dict(), critic_z.critic_z_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Launch the training</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'exchange-2_cpc_results.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-bdcd9ba91d33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#PREPROCESSING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exchange-2_cpc_results.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m#Splitting intro train and test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#TODO could be done in a more pythonic way\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'exchange-2_cpc_results.csv'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This part corresponds to the 3rd part of the above image.\n",
    "'''\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #PREPROCESSING\n",
    "    \n",
    "    dataset = pd.read_csv('exchange-2_cpc_results.csv')\n",
    "    #Splitting intro train and test\n",
    "    #TODO could be done in a more pythonic way\n",
    "    train_len = int(0.7 * dataset.shape[0])\n",
    "    dataset[0:train_len].to_csv('train_dataset.csv', index=False)\n",
    "    dataset[train_len:].to_csv('test_dataset.csv', index=False)\n",
    "\n",
    "    train_dataset = SignalDataset(path='train_dataset.csv')\n",
    "    test_dataset = SignalDataset(path='test_dataset.csv')\n",
    "    batch_size = 64 #define the batch size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, drop_last=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "    logging.info('Number of train datapoints is {}'.format(len(train_dataset)))\n",
    "    logging.info('Number of samples in train dataset {}'.format(len(train_dataset)))\n",
    "\n",
    "    lr = 1e-6\n",
    "\n",
    "    signal_shape = 100\n",
    "    latent_space_dim = 20\n",
    "    encoder_path = 'models/encoder.pt'\n",
    "    decoder_path = 'models/decoder.pt'\n",
    "    critic_x_path = 'models/critic_x.pt'\n",
    "    critic_z_path = 'models/critic_z.pt'\n",
    "    \n",
    "    encoder = model.Encoder(encoder_path, signal_shape)\n",
    "    decoder = model.Decoder(decoder_path, signal_shape)\n",
    "    critic_x = model.CriticX(critic_x_path, signal_shape)\n",
    "    critic_z = model.CriticZ(critic_z_path)\n",
    "\n",
    "    mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "    optim_enc = optim.Adam(encoder.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optim_dec = optim.Adam(decoder.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optim_cx = optim.Adam(critic_x.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optim_cz = optim.Adam(critic_z.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "    #launch the training\n",
    "    train(n_epochs=1)\n",
    "    \n",
    "    #Launch the testing\n",
    "    anomaly_detection.test(test_loader, encoder, decoder, critic_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
