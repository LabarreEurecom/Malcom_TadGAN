{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "contemporary-table",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red\" align=\"center\">TadGAN - Time series anomaly detection using GAN</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-internet",
   "metadata": {},
   "source": [
    "### Import needed/required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "naval-worth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-andorra",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Model's Architecture:</h1> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "opened-equipment",
   "metadata": {},
   "source": [
    "![image.png](./images/Model's_Architecture-Diagram.PNG) ![image-2.png](./images/Model's_Architecture-Description.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-dubai",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">TadGan Pseudo-Code:</h1> \n",
    "<h4 align=\"center\">(from the paper)</h4>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "palestinian-quick",
   "metadata": {},
   "source": [
    "![image.png](./images/TadGan-Algo.PNG) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-gather",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Define the Model</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-thriller",
   "metadata": {},
   "source": [
    "### Define the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "subjective-container",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder_path, signal_shape=100):\n",
    "        '''\n",
    "        encoder_path: ?? (idk its usefulness)\n",
    "        signal_shape = sequence length of the time serie\n",
    "        '''\n",
    "        super(Encoder, self).__init__()\n",
    "        self.signal_shape = signal_shape\n",
    "        \n",
    "        # lstm = long short-term memory\n",
    "        self.lstm = nn.LSTM(input_size=self.signal_shape, \n",
    "                            hidden_size=20, # Latent space dimension\n",
    "                            num_layers=1,   # Nb of layers\n",
    "                            bidirectional=True) # Forward & Backward\n",
    "        \n",
    "        self.dense = nn.Linear(in_features=40, out_features=20)\n",
    "        self.encoder_path = encoder_path\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(1, 64, self.signal_shape).float()\n",
    "        print(\"x.view\",x)\n",
    "        x, (hn, cn) = self.lstm(x)\n",
    "        print(self.lstm(x, (hn,cn)))\n",
    "        x = self.dense(x)\n",
    "        return (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-feeling",
   "metadata": {},
   "source": [
    "### Define the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "strange-array",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, decoder_path, signal_shape=100):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.signal_shape = signal_shape\n",
    "        self.lstm = nn.LSTM(input_size=20, hidden_size=64, num_layers=2, bidirectional=True)\n",
    "        self.dense = nn.Linear(in_features=128, out_features=self.signal_shape)\n",
    "        self.decoder_path = decoder_path\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, (hn, cn) = self.lstm(x)\n",
    "        x = self.dense(x)\n",
    "        return (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-story",
   "metadata": {},
   "source": [
    "### Define the CriticX"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "romantic-above",
   "metadata": {},
   "source": [
    "![image.png](./images/Wasserstein-Z-X-Cx.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "mobile-moses",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticX(nn.Module):\n",
    "    def __init__(self, critic_x_path, signal_shape=100):\n",
    "        super(CriticX, self).__init__()\n",
    "        self.signal_shape = signal_shape\n",
    "        self.dense1 = nn.Linear(in_features=self.signal_shape, out_features=20)\n",
    "        self.dense2 = nn.Linear(in_features=20, out_features=1)\n",
    "        self.critic_x_path = critic_x_path\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(1, 64, self.signal_shape).float()\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-banks",
   "metadata": {},
   "source": [
    "### Define the CriticZ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "middle-scratch",
   "metadata": {},
   "source": [
    "![image.png](./images/Wasserstein-X-Z-Cz.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "contrary-roberts",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticZ(nn.Module):\n",
    "    def __init__(self, critic_z_path):\n",
    "        super(CriticZ, self).__init__()\n",
    "        self.dense1 = nn.Linear(in_features=20, out_features=1)\n",
    "        self.critic_z_path = critic_z_path\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense1(x)\n",
    "        return (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-agency",
   "metadata": {},
   "source": [
    "### Define unroll_signal function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "foreign-things",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unroll_signal(self, x):\n",
    "    x = np.array(x).reshape(100)\n",
    "    return np.median(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "impaired-chicago",
   "metadata": {},
   "source": [
    "![image.png](./images/Cycle_Consistency_Loss.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-cement",
   "metadata": {},
   "source": [
    "### Define test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "exclusive-tuesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(self):\n",
    "    \"\"\"\n",
    "    Returns a dataframe with original value, reconstructed value, reconstruction error, critic score\n",
    "    \"\"\"\n",
    "    df = self.test_dataset.copy()\n",
    "    X_ = list()\n",
    "\n",
    "    RE = list()  #Reconstruction error\n",
    "    CS = list()  #Critic score\n",
    "\n",
    "    for i in range(0, df.shape[0]):\n",
    "        x = df.rolled_signal[i]\n",
    "        x = tf.reshape(x, (1, 100, 1))\n",
    "        z = encoder(x)\n",
    "        z = tf.expand_dims(z, axis=2)\n",
    "        x_ = decoder(z)\n",
    "\n",
    "        re = dtw_reconstruction_error(tf.squeeze(x_).numpy(), tf.squeeze(x).numpy()) #reconstruction error\n",
    "        cs = critic_x(x)\n",
    "        cs = tf.squeeze(cs).numpy()\n",
    "        RE.append(re)\n",
    "        CS.append(cs)\n",
    "\n",
    "        x_ = unroll_signal(x_)\n",
    "\n",
    "        X_.append(x_)\n",
    "\n",
    "    df['generated_signals'] = X_\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-patio",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Define the Anomaly Detection</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-islam",
   "metadata": {},
   "source": [
    "### Define the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "systematic-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, encoder, decoder, critic_x):\n",
    "    reconstruction_error = list()\n",
    "    critic_score = list()\n",
    "    y_true = list()\n",
    "\n",
    "    for batch, sample in enumerate(test_loader):\n",
    "        reconstructed_signal = decoder(encoder(sample['signal']))\n",
    "        reconstructed_signal = torch.squeeze(reconstructed_signal)\n",
    "\n",
    "        for i in range(0, 64):\n",
    "            x_ = reconstructed_signal[i].detach().numpy()\n",
    "            x = sample['signal'][i].numpy()\n",
    "            y_true.append(int(sample['anomaly'][i].detach()))\n",
    "            reconstruction_error.append(dtw_reconstruction_error(x, x_))\n",
    "        critic_score.extend(torch.squeeze(critic_x(sample['signal'])).detach().numpy())\n",
    "\n",
    "    reconstruction_error = stats.zscore(reconstruction_error)\n",
    "    critic_score = stats.zscore(critic_score)\n",
    "    anomaly_score = reconstruction_error * critic_score\n",
    "    y_predict = detect_anomaly(anomaly_score)\n",
    "    y_predict = prune_false_positive(y_predict, anomaly_score, change_threshold=0.1)\n",
    "    find_scores(y_true, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scientific-magnitude",
   "metadata": {},
   "source": [
    "### Other error metrics - point wise difference, Area difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "distant-piece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtw_reconstruction_error(x, x_):\n",
    "    n, m = x.shape[0], x_.shape[0]\n",
    "    dtw_matrix = np.zeros((n+1, m+1))\n",
    "    for i in range(n+1):\n",
    "        for j in range(m+1):\n",
    "            dtw_matrix[i, j] = np.inf\n",
    "    dtw_matrix[0, 0] = 0\n",
    "\n",
    "    for i in range(1, n+1):\n",
    "        for j in range(1, m+1):\n",
    "            cost = abs(x[i-1] - x_[j-1])\n",
    "            # take last min from a square box\n",
    "            last_min = np.min([dtw_matrix[i-1, j], dtw_matrix[i, j-1], dtw_matrix[i-1, j-1]])\n",
    "            dtw_matrix[i, j] = cost + last_min\n",
    "    return dtw_matrix[n][m]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-calvin",
   "metadata": {},
   "source": [
    "### Function to prune the false positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "italian-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_false_positive(is_anomaly, anomaly_score, change_threshold):\n",
    "    #The model might detect a high number of false positives.\n",
    "    #In such a scenario, pruning of the false positive is suggested.\n",
    "    #Method used is as described in the Section 5, part D Identifying Anomalous\n",
    "    #Sequence, sub-part - Mitigating False positives\n",
    "    #TODO code optimization\n",
    "    seq_details = []\n",
    "    delete_sequence = 0\n",
    "    start_position = 0\n",
    "    end_position = 0\n",
    "    max_seq_element = anomaly_score[0]\n",
    "    for i in range(1, len(is_anomaly)):\n",
    "        if i+1 == len(is_anomaly):\n",
    "            seq_details.append([start_position, i, max_seq_element, delete_sequence])\n",
    "        elif is_anomaly[i] == 1 and is_anomaly[i+1] == 0:\n",
    "            end_position = i\n",
    "            seq_details.append([start_position, end_position, max_seq_element, delete_sequence])\n",
    "        elif is_anomaly[i] == 1 and is_anomaly[i-1] == 0:\n",
    "            start_position = i\n",
    "            max_seq_element = anomaly_score[i]\n",
    "        if is_anomaly[i] == 1 and is_anomaly[i-1] == 1 and anomaly_score[i] > max_seq_element:\n",
    "            max_seq_element = anomaly_score[i]\n",
    "\n",
    "    max_elements = list()\n",
    "    for i in range(0, len(seq_details)):\n",
    "        max_elements.append(seq_details[i][2])\n",
    "\n",
    "    max_elements.sort(reverse=True)\n",
    "    max_elements = np.array(max_elements)\n",
    "    change_percent = abs(max_elements[1:] - max_elements[:-1]) / max_elements[1:]\n",
    "\n",
    "    #Appending 0 for the 1 st element which is not change percent\n",
    "    delete_seq = np.append(np.array([0]), change_percent < change_threshold)\n",
    "\n",
    "    #Mapping max element and seq details\n",
    "    for i, max_elt in enumerate(max_elements):\n",
    "        for j in range(0, len(seq_details)):\n",
    "            if seq_details[j][2] == max_elt:\n",
    "                seq_details[j][3] = delete_seq[i]\n",
    "\n",
    "    for seq in seq_details:\n",
    "        if seq[3] == 1: #Delete sequence\n",
    "            is_anomaly[seq[0]:seq[1]+1] = [0] * (seq[1] - seq[0] + 1)\n",
    " \n",
    "    return is_anomaly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-seeking",
   "metadata": {},
   "source": [
    "### Function to detect anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "verified-header",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomaly(anomaly_score):\n",
    "    window_size = len(anomaly_score) // 3\n",
    "    step_size = len(anomaly_score) // (3 * 10)\n",
    "\n",
    "    is_anomaly = np.zeros(len(anomaly_score))\n",
    "\n",
    "    for i in range(0, len(anomaly_score) - window_size, step_size):\n",
    "        window_elts = anomaly_score[i:i+window_size]\n",
    "        window_mean = np.mean(window_elts)\n",
    "        window_std = np.std(window_mean)\n",
    "\n",
    "        for j, elt in enumerate(window_elts):\n",
    "            if (window_mean - 3 * window_std) < elt < (window_mean + 3 * window_std):\n",
    "                is_anomaly[i + j] = 0\n",
    "            else:\n",
    "                is_anomaly[i + j] = 1\n",
    "\n",
    "    return is_anomaly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-appeal",
   "metadata": {},
   "source": [
    "### Function to find score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-whole",
   "metadata": {},
   "source": [
    "<u>Reminder</u>:\n",
    "\n",
    "$ \\boxed{ Accuracy = \\frac {T_{P}+T_{N}} {N_{total}} } $.\n",
    "<b style=\"color:red\">ACCURACY used to know HOW WELL the PREDICTION IS</b>.\n",
    "\n",
    "$ \\boxed{ Precision = \\frac {T_{P}} {T_{P}+F_{P}} } $.\n",
    "<b style=\"color:red\">PRECISION used to know HOW WELL/ACCURATE a POSITIVE IS DETERMINED</b>.\n",
    "\n",
    "$ \\boxed{ Recall/Sensitivity = \\frac {T_{P}} {T_{P}+F_{N}} } $.\n",
    "<b style=\"color:red\">RECALL or SENSITIVITY used to know HOW MUCH POSITIVES ARE DETERMINED</b>.\n",
    "\n",
    "$ \\boxed{ F1  score = \\frac {2} {Precision^{-1} + Recall^{-1}} = \\frac {2 . Precision . Recall} {Precision + Recall} = \\frac {T_{P}} {T_{P} + \\frac{1}{2} (F_{N}+F_{P})} } $."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "wrapped-reflection",
   "metadata": {},
   "source": [
    "### Here\n",
    "\n",
    "![image.png](./images/TP_FP_FN.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "refined-music",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_scores(y_true, y_predict):\n",
    "    tp = tn = fp = fn = 0\n",
    "\n",
    "    for i in range(0, len(y_true)):\n",
    "        if y_true[i] == 1 and y_predict[i] == 1:\n",
    "            tp += 1\n",
    "        elif y_true[i] == 1 and y_predict[i] == 0:\n",
    "            fn += 1\n",
    "        elif y_true[i] == 0 and y_predict[i] == 0:\n",
    "            tn += 1\n",
    "        elif y_true[i] == 0 and y_predict[i] == 1:\n",
    "            fp += 1\n",
    "\n",
    "    print ('Accuracy {:.2f}'.format((tp + tn)/(len(y_true))))\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    print ('Precision {:.2f}'.format(precision))\n",
    "    print ('Recall {:.2f}'.format(recall))\n",
    "    print ('F1 Score {:.2f}'.format(2 * precision * recall / (precision + recall)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-audit",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Apply the model</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-companion",
   "metadata": {},
   "source": [
    "### To debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "third-knock",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='train.log', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-gentleman",
   "metadata": {},
   "source": [
    "### Define the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "elder-embassy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignalDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.signal_df = pd.read_csv(path)\n",
    "        self.signal_columns = self.make_signal_list()\n",
    "        self.make_rolling_signals()\n",
    "\n",
    "    def make_signal_list(self):\n",
    "        signal_list = list()\n",
    "        for i in range(-50, 50):\n",
    "            signal_list.append('signal'+str(i))\n",
    "        return signal_list\n",
    "\n",
    "    def make_rolling_signals(self):\n",
    "        for i in range(-50, 50):\n",
    "            self.signal_df['signal'+str(i)] = self.signal_df['signal'].shift(i)\n",
    "        self.signal_df = self.signal_df.dropna()\n",
    "        self.signal_df = self.signal_df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.signal_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.signal_df.loc[idx]\n",
    "        x = row[self.signal_columns].values.astype(float)\n",
    "        x = torch.from_numpy(x)\n",
    "        return {'signal':x, 'anomaly':row['anomaly']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-arthritis",
   "metadata": {},
   "source": [
    "### Define the CriticX training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "working-volunteer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def critic_x_iteration(sample):\n",
    "    optim_cx.zero_grad()\n",
    "\n",
    "    x = sample['signal'].view(1, batch_size, signal_shape)\n",
    "    valid_x = critic_x(x)\n",
    "    valid_x = torch.squeeze(valid_x)\n",
    "    critic_score_valid_x = torch.mean(torch.ones(valid_x.shape) * valid_x) #Wasserstein Loss\n",
    "\n",
    "    #The sampled z are the anomalous points - points deviating from actual distribution of z (obtained through encoding x)\n",
    "    z = torch.empty(1, batch_size, latent_space_dim).uniform_(0, 1)\n",
    "    x_ = decoder(z)\n",
    "    fake_x = critic_x(x_)\n",
    "    fake_x = torch.squeeze(fake_x)\n",
    "    critic_score_fake_x = torch.mean(torch.ones(fake_x.shape) * fake_x)  #Wasserstein Loss\n",
    "\n",
    "    alpha = torch.rand(x.shape)\n",
    "    ix = Variable(alpha * x + (1 - alpha) * x_) #Random Weighted Average\n",
    "    ix.requires_grad_(True)\n",
    "    v_ix = critic_x(ix)\n",
    "    v_ix.mean().backward()\n",
    "    gradients = ix.grad\n",
    "    #Gradient Penalty Loss\n",
    "    gp_loss = torch.sqrt(torch.sum(torch.square(gradients).view(-1)))\n",
    "\n",
    "    #Critic has to maximize Cx(Valid X) - Cx(Fake X).\n",
    "    #Maximizing the above is same as minimizing the negative.\n",
    "    wl = critic_score_fake_x - critic_score_valid_x\n",
    "    loss = wl + gp_loss\n",
    "    loss.backward()\n",
    "    optim_cx.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-politics",
   "metadata": {},
   "source": [
    "### Define the CriticZ training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bored-bubble",
   "metadata": {},
   "outputs": [],
   "source": [
    "def critic_z_iteration(sample):\n",
    "    optim_cz.zero_grad()\n",
    "\n",
    "    x = sample['signal'].view(1, batch_size, signal_shape)\n",
    "    z = encoder(x)\n",
    "    valid_z = critic_z(z)\n",
    "    valid_z = torch.squeeze(valid_z)\n",
    "    critic_score_valid_z = torch.mean(torch.ones(valid_z.shape) * valid_z)\n",
    "\n",
    "    z_ = torch.empty(1, batch_size, latent_space_dim).uniform_(0, 1)\n",
    "    fake_z = critic_z(z_)\n",
    "    fake_z = torch.squeeze(fake_z)\n",
    "    critic_score_fake_z = torch.mean(torch.ones(fake_z.shape) * fake_z) #Wasserstein Loss\n",
    "\n",
    "    wl = critic_score_fake_z - critic_score_valid_z\n",
    "\n",
    "    alpha = torch.rand(z.shape)\n",
    "    iz = Variable(alpha * z + (1 - alpha) * z_) #Random Weighted Average\n",
    "    iz.requires_grad_(True)\n",
    "    v_iz = critic_z(iz)\n",
    "    v_iz.mean().backward()\n",
    "    gradients = iz.grad\n",
    "    gp_loss = torch.sqrt(torch.sum(torch.square(gradients).view(-1)))\n",
    "\n",
    "    loss = wl + gp_loss\n",
    "    loss.backward()\n",
    "    optim_cz.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-movie",
   "metadata": {},
   "source": [
    "### Define the Encoder training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "speaking-witch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_iteration(sample):\n",
    "    optim_enc.zero_grad()\n",
    "\n",
    "    x = sample['signal'].view(1, batch_size, signal_shape)\n",
    "    valid_x = critic_x(x)\n",
    "    valid_x = torch.squeeze(valid_x)\n",
    "    critic_score_valid_x = torch.mean(torch.ones(valid_x.shape) * valid_x) #Wasserstein Loss\n",
    "\n",
    "    z = torch.empty(1, batch_size, latent_space_dim).uniform_(0, 1)\n",
    "    x_ = decoder(z)\n",
    "    fake_x = critic_x(x_)\n",
    "    fake_x = torch.squeeze(fake_x)\n",
    "    critic_score_fake_x = torch.mean(torch.ones(fake_x.shape) * fake_x)\n",
    "\n",
    "    enc_z = encoder(x)\n",
    "    gen_x = decoder(enc_z)\n",
    "\n",
    "    mse = mse_loss(x.float(), gen_x.float())\n",
    "    loss_enc = mse + critic_score_valid_x - critic_score_fake_x\n",
    "    loss_enc.backward(retain_graph=True)\n",
    "    optim_enc.step()\n",
    "\n",
    "    return loss_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-summit",
   "metadata": {},
   "source": [
    "### Define the Decoder training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "running-soldier",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_iteration(sample):\n",
    "    optim_dec.zero_grad()\n",
    "\n",
    "    x = sample['signal'].view(1, batch_size, signal_shape)\n",
    "    z = encoder(x)\n",
    "    valid_z = critic_z(z)\n",
    "    valid_z = torch.squeeze(valid_z)\n",
    "    critic_score_valid_z = torch.mean(torch.ones(valid_z.shape) * valid_z)\n",
    "\n",
    "    z_ = torch.empty(1, batch_size, latent_space_dim).uniform_(0, 1)\n",
    "    fake_z = critic_z(z_)\n",
    "    fake_z = torch.squeeze(fake_z)\n",
    "    critic_score_fake_z = torch.mean(torch.ones(fake_z.shape) * fake_z)\n",
    "\n",
    "    enc_z = encoder(x)\n",
    "    gen_x = decoder(enc_z)\n",
    "\n",
    "    mse = mse_loss(x.float(), gen_x.float())\n",
    "    loss_dec = mse + critic_score_valid_z - critic_score_fake_z\n",
    "    loss_dec.backward(retain_graph=True)\n",
    "    optim_dec.step()\n",
    "\n",
    "    return loss_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-incidence",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "strong-initial",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs=2000):\n",
    "    logging.debug('Starting training')\n",
    "    cx_epoch_loss = list()\n",
    "    cz_epoch_loss = list()\n",
    "    encoder_epoch_loss = list()\n",
    "    decoder_epoch_loss = list()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        logging.debug('Epoch {}'.format(epoch))\n",
    "        n_critics = 5\n",
    "\n",
    "        cx_nc_loss = list()\n",
    "        cz_nc_loss = list()\n",
    "\n",
    "        for i in range(n_critics):\n",
    "            cx_loss = list()\n",
    "            cz_loss = list()\n",
    "\n",
    "            for batch, sample in enumerate(train_loader):\n",
    "                loss = critic_x_iteration(sample)\n",
    "                cx_loss.append(loss)\n",
    "\n",
    "                loss = critic_z_iteration(sample)\n",
    "                cz_loss.append(loss)\n",
    "\n",
    "            cx_nc_loss.append(torch.mean(torch.tensor(cx_loss)))\n",
    "            cz_nc_loss.append(torch.mean(torch.tensor(cz_loss)))\n",
    "\n",
    "        logging.debug('Critic training done in epoch {}'.format(epoch))\n",
    "        encoder_loss = list()\n",
    "        decoder_loss = list()\n",
    "\n",
    "        for batch, sample in enumerate(train_loader):\n",
    "            enc_loss = encoder_iteration(sample)\n",
    "            dec_loss = decoder_iteration(sample)\n",
    "            encoder_loss.append(enc_loss)\n",
    "            decoder_loss.append(dec_loss)\n",
    "\n",
    "        cx_epoch_loss.append(torch.mean(torch.tensor(cx_nc_loss)))\n",
    "        cz_epoch_loss.append(torch.mean(torch.tensor(cz_nc_loss)))\n",
    "        encoder_epoch_loss.append(torch.mean(torch.tensor(encoder_loss)))\n",
    "        decoder_epoch_loss.append(torch.mean(torch.tensor(decoder_loss)))\n",
    "        logging.debug('Encoder decoder training done in epoch {}'.format(epoch))\n",
    "        logging.debug('critic x loss {:.3f} critic z loss {:.3f} \\nencoder loss {:.3f} decoder loss {:.3f}\\n'.format(cx_epoch_loss[-1], cz_epoch_loss[-1], encoder_epoch_loss[-1], decoder_epoch_loss[-1]))\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            torch.save(encoder.state_dict(), encoder.encoder_path)\n",
    "            torch.save(decoder.state_dict(), decoder.decoder_path)\n",
    "            torch.save(critic_x.state_dict(), critic_x.critic_x_path)\n",
    "            torch.save(critic_z.state_dict(), critic_z.critic_z_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-coupon",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Launch the training</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-mileage",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dataset = pd.read_csv('exchange-2_cpc_results.csv')\n",
    "    #Splitting intro train and test\n",
    "    #TODO could be done in a more pythonic way\n",
    "    train_len = int(0.7 * dataset.shape[0])\n",
    "    dataset[0:train_len].to_csv('train_dataset.csv', index=False)\n",
    "    dataset[train_len:].to_csv('test_dataset.csv', index=False)\n",
    "\n",
    "    train_dataset = SignalDataset(path='train_dataset.csv')\n",
    "    test_dataset = SignalDataset(path='test_dataset.csv')\n",
    "    batch_size = 64\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, drop_last=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "    logging.info('Number of train datapoints is {}'.format(len(train_dataset)))\n",
    "    logging.info('Number of samples in train dataset {}'.format(len(train_dataset)))\n",
    "\n",
    "    lr = 1e-6\n",
    "\n",
    "    signal_shape = 100\n",
    "    latent_space_dim = 20\n",
    "    encoder_path = 'models/encoder.pt'\n",
    "    decoder_path = 'models/decoder.pt'\n",
    "    critic_x_path = 'models/critic_x.pt'\n",
    "    critic_z_path = 'models/critic_z.pt'\n",
    "    \n",
    "    encoder = model.Encoder(encoder_path, signal_shape)\n",
    "    decoder = model.Decoder(decoder_path, signal_shape)\n",
    "    critic_x = model.CriticX(critic_x_path, signal_shape)\n",
    "    critic_z = model.CriticZ(critic_z_path)\n",
    "\n",
    "    mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "    optim_enc = optim.Adam(encoder.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optim_dec = optim.Adam(decoder.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optim_cx = optim.Adam(critic_x.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optim_cz = optim.Adam(critic_z.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "    train(n_epochs=1)\n",
    "\n",
    "    anomaly_detection.test(test_loader, encoder, decoder, critic_x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
