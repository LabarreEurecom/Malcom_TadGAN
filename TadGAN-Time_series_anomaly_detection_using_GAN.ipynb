{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "consecutive-vision",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red\" align=\"center\">TadGAN - Time series anomaly detection using GAN</h1>\n",
    "\n",
    "### Import needed/required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "under-insert",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-electricity",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Model's Architecture:</h1> \n",
    "\n",
    "![image.png](./images/Model's_Architecture-Diagram.PNG) ![image-2.png](./images/Model's_Architecture-Description.PNG)\n",
    "\n",
    "<h1 align=\"center\">TadGan Pseudo-Code:</h1> \n",
    "<h4 align=\"center\">(from the paper)</h4>\n",
    "\n",
    "![image.png](./images/TadGan-Algo.PNG) \n",
    "\n",
    "X is an univariate series with $x_{i}$ scalars. \n",
    "\n",
    "<h1 align=\"center\">Define the Model</h1>\n",
    "\n",
    "### Define the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "recorded-dayton",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder_path, signal_shape=100):\n",
    "        '''\n",
    "        encoder_path: ?? (idk its usefulness)\n",
    "        signal_shape = sequence length of the time serie\n",
    "        '''\n",
    "        super(Encoder, self).__init__()\n",
    "        self.signal_shape = signal_shape\n",
    "        \n",
    "        # lstm = long short-term memory\n",
    "        self.lstm = nn.LSTM(input_size=self.signal_shape, \n",
    "                            hidden_size=20, # Latent space dimension\n",
    "                            num_layers=1,   # Nb of layers\n",
    "                            bidirectional=True) # Forward & Backward\n",
    "        \n",
    "        self.dense = nn.Linear(in_features=40, out_features=20) \n",
    "        self.encoder_path = encoder_path\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(1, 64, self.signal_shape).float()\n",
    "#         print(\"x.view\",x,\"\\n\\n np.shape(x)\",np.shape(x))\n",
    "        x, (hn, cn) = self.lstm(x)\n",
    "        #print(self.lstm(x, (hn,cn))) # Do not launch, the error comes from here\n",
    "        x = self.dense(x)\n",
    "        return (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-passing",
   "metadata": {},
   "source": [
    "### Define the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial-police",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, decoder_path, signal_shape=100):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.signal_shape = signal_shape\n",
    "        self.lstm = nn.LSTM(input_size=20, \n",
    "                            hidden_size=64, \n",
    "                            num_layers=2, \n",
    "                            bidirectional=True)\n",
    "        self.dense = nn.Linear(in_features=128, out_features=self.signal_shape)\n",
    "        self.decoder_path = decoder_path\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, (hn, cn) = self.lstm(x)\n",
    "        x = self.dense(x)\n",
    "        return (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-debate",
   "metadata": {},
   "source": [
    "## THE CRITICS\n",
    "\n",
    "In the following code we introduce 2  two adversarial **critics** (aka discriminators) Cx and Cz. \n",
    "- **Cx**: distinguish between the real time series sequences from X and the generated time series sequences from G(z)\n",
    "- **Cz**: measure the performance of the mapping into latent space. \n",
    "\n",
    "In other words, G is trying to fool Cx by generating real-looking sequences. \n",
    " \n",
    "Thus, our high-level objective consists of two terms: \n",
    "    (1) Wasserstein losses, to match the distribution of generated time series sequences to the data distribution in\n",
    "the target domain\n",
    "    (2) cycle consistency losses, to prevent the contradiction between E and G (encoder and decoder)\n",
    "    \n",
    "### Define the CriticX :\n",
    "\n",
    "REMINDER: The goal of Cx is to distinguish between the real time series sequences from X and the generated time series sequences from G(z) by using Wasserstein loss\n",
    "\n",
    "![image.png](./images/Wasserstein-Z-X-Cx.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "boring-psychology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Look more in detailed how it is defined in term of coding\n",
    "class CriticX(nn.Module):\n",
    "    def __init__(self, critic_x_path, signal_shape=100):\n",
    "        super(CriticX, self).__init__()\n",
    "        self.signal_shape = signal_shape\n",
    "        self.dense1 = nn.Linear(in_features=self.signal_shape, out_features=20)\n",
    "        self.dense2 = nn.Linear(in_features=20, out_features=1)\n",
    "        self.critic_x_path = critic_x_path\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(1, 64, self.signal_shape).float()\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-falls",
   "metadata": {},
   "source": [
    "### Define the CriticZ\n",
    "\n",
    "Cz measures the performance of the mapping into latent space. \n",
    "\n",
    "![image.png](./images/Wasserstein-X-Z-Cz.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "better-diameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticZ(nn.Module):\n",
    "    def __init__(self, critic_z_path):\n",
    "        super(CriticZ, self).__init__()\n",
    "        self.dense1 = nn.Linear(in_features=20, out_features=1)\n",
    "        self.critic_z_path = critic_z_path\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense1(x)\n",
    "        return (x)\n",
    "    \n",
    "    def unroll_signal(self, x):\n",
    "        x = np.array(x).reshape(100)\n",
    "        return np.median(x)\n",
    "    \n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        Returns a dataframe with original value, reconstructed value, reconstruction error, critic score\n",
    "        \"\"\"\n",
    "        df = self.test_dataset.copy()\n",
    "        X_ = list()\n",
    "\n",
    "        RE = list()  #Reconstruction error\n",
    "        CS = list()  #Critic score\n",
    "\n",
    "        for i in range(0, df.shape[0]):\n",
    "            x = df.rolled_signal[i]\n",
    "            x = tf.reshape(x, (1, 100, 1))\n",
    "            z = encoder(x) #E(x)\n",
    "            z = tf.expand_dims(z, axis=2)\n",
    "            x_ = decoder(z) #G(E(x))\n",
    "\n",
    "            re = dtw_reconstruction_error(tf.squeeze(x_).numpy(), tf.squeeze(x).numpy()) #reconstruction error\n",
    "            cs = critic_x(x)\n",
    "            cs = tf.squeeze(cs).numpy()\n",
    "            RE.append(re)\n",
    "            CS.append(cs)\n",
    "\n",
    "            x_ = unroll_signal(x_)\n",
    "\n",
    "            X_.append(x_)\n",
    "\n",
    "        df['generated_signals'] = X_\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-blast",
   "metadata": {},
   "source": [
    "![image.png](./images/Cycle_Consistency_Loss.PNG)\n",
    "\n",
    "### Finally, we have\n",
    "\n",
    "![image.png](./images/Full_objective.PNG)\n",
    "\n",
    "### To reconstruct the signal\n",
    "\n",
    "![image.png](./images/Signal_reconstruction.PNG)\n",
    "\n",
    "<h1 align=\"center\">Define the Anomaly Detection</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-faith",
   "metadata": {},
   "source": [
    "### Other error metrics - point wise difference, Area difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eleven-plasma",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtw_reconstruction_error(x, x_):\n",
    "    n, m = x.shape[0], x_.shape[0]\n",
    "    dtw_matrix = np.zeros((n+1, m+1))\n",
    "    for i in range(n+1):\n",
    "        for j in range(m+1):\n",
    "            dtw_matrix[i, j] = np.inf\n",
    "    dtw_matrix[0, 0] = 0\n",
    "\n",
    "    for i in range(1, n+1):\n",
    "        for j in range(1, m+1):\n",
    "            cost = abs(x[i-1] - x_[j-1])\n",
    "            # take last min from a square box\n",
    "            last_min = np.min([dtw_matrix[i-1, j], dtw_matrix[i, j-1], dtw_matrix[i-1, j-1]])\n",
    "            dtw_matrix[i, j] = cost + last_min\n",
    "    return dtw_matrix[n][m]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-northwest",
   "metadata": {},
   "source": [
    "### Function to prune the false positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "reflected-ballet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_false_positive(is_anomaly, anomaly_score, change_threshold):\n",
    "    #The model might detect a high number of false positives.\n",
    "    #In such a scenario, pruning of the false positive is suggested.\n",
    "    #Method used is as described in the Section 5, part D Identifying Anomalous\n",
    "    #Sequence, sub-part - Mitigating False positives\n",
    "    #TODO code optimization\n",
    "    seq_details = []\n",
    "    delete_sequence = 0\n",
    "    start_position = 0\n",
    "    end_position = 0\n",
    "    max_seq_element = anomaly_score[0]\n",
    "    for i in range(1, len(is_anomaly)):\n",
    "        if i+1 == len(is_anomaly):\n",
    "            seq_details.append([start_position, i, max_seq_element, delete_sequence])\n",
    "        elif is_anomaly[i] == 1 and is_anomaly[i+1] == 0:\n",
    "            end_position = i\n",
    "            seq_details.append([start_position, end_position, max_seq_element, delete_sequence])\n",
    "        elif is_anomaly[i] == 1 and is_anomaly[i-1] == 0:\n",
    "            start_position = i\n",
    "            max_seq_element = anomaly_score[i]\n",
    "        if is_anomaly[i] == 1 and is_anomaly[i-1] == 1 and anomaly_score[i] > max_seq_element:\n",
    "            max_seq_element = anomaly_score[i]\n",
    "\n",
    "    max_elements = list()\n",
    "    for i in range(0, len(seq_details)):\n",
    "        max_elements.append(seq_details[i][2])\n",
    "\n",
    "    max_elements.sort(reverse=True)\n",
    "    max_elements = np.array(max_elements)\n",
    "    change_percent = abs(max_elements[1:] - max_elements[:-1]) / max_elements[1:]\n",
    "\n",
    "    #Appending 0 for the 1 st element which is not change percent\n",
    "    delete_seq = np.append(np.array([0]), change_percent < change_threshold)\n",
    "\n",
    "    #Mapping max element and seq details\n",
    "    for i, max_elt in enumerate(max_elements):\n",
    "        for j in range(0, len(seq_details)):\n",
    "            if seq_details[j][2] == max_elt:\n",
    "                seq_details[j][3] = delete_seq[i]\n",
    "\n",
    "    for seq in seq_details:\n",
    "        if seq[3] == 1: #Delete sequence\n",
    "            is_anomaly[seq[0]:seq[1]+1] = [0] * (seq[1] - seq[0] + 1)\n",
    " \n",
    "    return is_anomaly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-merit",
   "metadata": {},
   "source": [
    "### Function to detect anomaly\n",
    "\n",
    "Here we create the training sample by subdividing the original time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "original-compatibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomaly(anomaly_score):\n",
    "    '''\n",
    "    define the training sample by introducing a sliding window \n",
    "    a size t =len(anomaly_score) // 3 \n",
    "    and a stepsize s = len(anomaly_score) // (3 * 10)\n",
    "    \n",
    "    Thus the original time series is divided into N sub-sequences X = {(x{1...t}i)}N{i=1}, where N =(T âˆ’t)/s\n",
    "    '''\n",
    "    window_size = len(anomaly_score) // 3\n",
    "    step_size = len(anomaly_score) // (3 * 10)\n",
    "\n",
    "    is_anomaly = np.zeros(len(anomaly_score))\n",
    "\n",
    "    for i in range(0, len(anomaly_score) - window_size, step_size):\n",
    "        window_elts = anomaly_score[i:i+window_size] # sliding window of size window_size\n",
    "        window_mean = np.mean(window_elts)\n",
    "        window_std = np.std(window_mean)\n",
    "\n",
    "        for j, elt in enumerate(window_elts):\n",
    "            if (window_mean - 3 * window_std) < elt < (window_mean + 3 * window_std):\n",
    "                is_anomaly[i + j] = 0\n",
    "            else:\n",
    "                is_anomaly[i + j] = 1\n",
    "\n",
    "    return is_anomaly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-sandwich",
   "metadata": {},
   "source": [
    "### Function to find score\n",
    "\n",
    "<u>Reminder</u>:\n",
    "\n",
    "$ \\boxed{ Accuracy = \\frac {T_{P}+T_{N}} {N_{total}} } $.\n",
    "<b style=\"color:red\">ACCURACY used to know HOW WELL the PREDICTION IS</b>.\n",
    "\n",
    "$ \\boxed{ Precision = \\frac {T_{P}} {T_{P}+F_{P}} } $.\n",
    "<b style=\"color:red\">PRECISION used to know HOW WELL/ACCURATE a POSITIVE IS DETERMINED</b>.\n",
    "\n",
    "$ \\boxed{ Recall/Sensitivity = \\frac {T_{P}} {T_{P}+F_{N}} } $.\n",
    "<b style=\"color:red\">RECALL or SENSITIVITY used to know HOW MUCH POSITIVES ARE DETERMINED</b>.\n",
    "\n",
    "$ \\boxed{ F1  score = \\frac {2} {Precision^{-1} + Recall^{-1}} = \\frac {2 . Precision . Recall} {Precision + Recall} = \\frac {T_{P}} {T_{P} + \\frac{1}{2} (F_{N}+F_{P})} } $.\n",
    "\n",
    "### Here\n",
    "\n",
    "![image.png](./images/TP_FP_FN.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "divine-stephen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_scores(y_true, y_predict):\n",
    "    tp = tn = fp = fn = 0\n",
    "\n",
    "    for i in range(0, len(y_true)):\n",
    "        if y_true[i] == 1 and y_predict[i] == 1:\n",
    "            tp += 1\n",
    "        elif y_true[i] == 1 and y_predict[i] == 0:\n",
    "            fn += 1\n",
    "        elif y_true[i] == 0 and y_predict[i] == 0:\n",
    "            tn += 1\n",
    "        elif y_true[i] == 0 and y_predict[i] == 1:\n",
    "            fp += 1\n",
    "    print(\"tp\",tp,\"\\n\\ntn\",tn,\"\\n\\nfp\",fp,\"\\n\\nfn\",fn)\n",
    "    print ('Accuracy {:.2f}'.format((tp + tn)/(len(y_true))))\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    print ('Precision {:.2f}'.format(precision))\n",
    "    print ('Recall {:.2f}'.format(recall))\n",
    "    print ('F1 Score {:.2f}'.format(2 * precision * recall / (precision + recall)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-logic",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Apply the model</h1>\n",
    "\n",
    "### To debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "loaded-running",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='train.log', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-dylan",
   "metadata": {},
   "source": [
    "### Define the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "united-jacksonville",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignalDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.signal_df = pd.read_csv(path)\n",
    "        self.signal_columns = self.make_signal_list()\n",
    "        self.make_rolling_signals()\n",
    "\n",
    "    def make_signal_list(self):\n",
    "        signal_list = list()\n",
    "        for i in range(-50, 50):\n",
    "            signal_list.append('signal'+str(i))\n",
    "        return signal_list\n",
    "\n",
    "    def make_rolling_signals(self):\n",
    "        for i in range(-50, 50):\n",
    "            self.signal_df['signal'+str(i)] = self.signal_df['signal'].shift(i)\n",
    "        self.signal_df = self.signal_df.dropna()\n",
    "        self.signal_df = self.signal_df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.signal_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.signal_df.loc[idx]\n",
    "        x = row[self.signal_columns].values.astype(float)\n",
    "        x = torch.from_numpy(x)\n",
    "        return {'signal':x, 'anomaly':row['anomaly']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-trunk",
   "metadata": {},
   "source": [
    "## Define the CriticX training (compute the wassertein loss for critic X)\n",
    "\n",
    "**GP** stands for **Gradient Penalty**\n",
    "\n",
    "From a sample a data (*subdivision of the original time series*) we generate $Z={(z_{1},z_{2},...,z_{k})}$ from a random space following normal distribution, where $k$ denotes the dimension of the latent space. Here we **encode $x$ in $z$**.\n",
    "\n",
    "- STEP 1 : Resizing x\n",
    "- STEP 2 : Estimate the Anomaly Scores with Critic X Outputs\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{m} \\sum_{i=1}^m C_{x}(x_{i}) \\\\\n",
    "\\end{equation}\n",
    "\n",
    "- STEP 3 : Estimate the Anomaly Scores with Critic Z Outputs\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{m} \\sum_{i=1}^m C_{x}(G(z_{i})) \\\\\n",
    "\\end{equation}\n",
    "\n",
    "- STEP 4 : Compute the gradient penalty\n",
    "\\begin{equation}\n",
    "gp(x_{i}, G(z_{i})) \\\\\n",
    "\\end{equation}\n",
    "\n",
    "- STEP 5 :\n",
    "\n",
    "$$step 2 - step 3 + step 4$$\n",
    "\n",
    "- STEP 6 : Compute the gradient on $w_{G}$ and $w_{E}$\n",
    "- STEP 7 : Update the weights $w_{C_{x}}$\n",
    "\n",
    "\\begin{equation}\n",
    "w_{C_{x}} =w_{C_{x}} + stepSize* adam(w_{C_{x}}, STEP5)\\\\\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "reported-memorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "def critic_x_iteration(sample):\n",
    "    optim_cx.zero_grad()\n",
    "    \n",
    "    # STEP 1\n",
    "    x = sample['signal'].view(1, batch_size, signal_shape)\n",
    "    \n",
    "    # STEP 2\n",
    "    valid_x = critic_x(x)\n",
    "    valid_x = torch.squeeze(valid_x)\n",
    "    critic_score_valid_x = torch.mean(torch.ones(valid_x.shape) * valid_x) #Wasserstein Loss\n",
    "    \n",
    "    # STEP 3\n",
    "    #The sampled z are the anomalous points - points deviating from actual distribution of z (obtained through encoding x)\n",
    "    z = torch.empty(1, batch_size, latent_space_dim).uniform_(0, 1)\n",
    "    x_ = decoder(z)\n",
    "    fake_x = critic_x(x_)\n",
    "    fake_x = torch.squeeze(fake_x)\n",
    "    critic_score_fake_x = torch.mean(torch.ones(fake_x.shape) * fake_x)  #Wasserstein Loss\n",
    "    \n",
    "    \n",
    "    # STEP 4\n",
    "    alpha = torch.rand(x.shape)\n",
    "    ix = Variable(alpha * x + (1 - alpha) * x_) #Random Weighted Average\n",
    "    ix.requires_grad_(True)\n",
    "    v_ix = critic_x(ix)\n",
    "    v_ix.mean().backward()\n",
    "    gradients = ix.grad\n",
    "    #Gradient Penalty Loss\n",
    "    gp_loss = torch.sqrt(torch.sum(torch.square(gradients).view(-1)))\n",
    "\n",
    "    \n",
    "    # STEP 5\n",
    "    #Critic has to maximize Cx(Valid X) - Cx(Fake X).\n",
    "    #Maximizing the above is same as minimizing the negative.\n",
    "    wl = critic_score_fake_x - critic_score_valid_x\n",
    "    loss = wl + gp_loss\n",
    "    \n",
    "    # STEP 6\n",
    "    loss.backward()\n",
    "    optim_cx.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-profession",
   "metadata": {},
   "source": [
    "## Define the CriticZ training (compute the wassertein loss for critic Z)\n",
    "\n",
    "From a sample a data (*subdivision of the original time series*) we generate $Z={(z_{1},z_{2},...,z_{k})}$ from a random space following normal distribution, where $k$ denotes the dimension of the latent space. Here we **encode $x$ in $z$**.\n",
    "\n",
    "- STEP 1 : Resizing x\n",
    "- STEP 2 :\n",
    "Estimate the Anomaly Scores with Critic Z Outputs\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{m} \\sum_{i=1}^m C_{z}(z_{i}) \\\\\n",
    "\\end{equation}\n",
    "\n",
    "- STEP 3 : Estimate the Anomaly Scores with Critic Z Outputs\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{m} \\sum_{i=1}^m C_{z}(E(x_{i})) \\\\\n",
    "\\end{equation}\n",
    "\n",
    "- STEP 4 : Compute the gradient penalty\n",
    "\\begin{equation}\n",
    "gp(x_{i}, E(z_{i})) \\\\\n",
    "\\end{equation}\n",
    "\n",
    "- STEP 5 :\n",
    "\n",
    "$$step 2 - step 3 + step 4$$\n",
    "\n",
    "- STEP 6 : Compute the gradient on $w_{G}$ and $w_{E}$\n",
    "- STEP 7 : Update the weights $w_{C_{z}}$\n",
    "\n",
    "\\begin{equation}\n",
    "w_{C_{z}} =w_{C_{z}} + stepSize* adam(w_{C_{z}}, step5)\\\\\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "australian-intervention",
   "metadata": {},
   "outputs": [],
   "source": [
    "def critic_z_iteration(sample):\n",
    "    optim_cz.zero_grad()\n",
    "    \n",
    "    # STEP 1\n",
    "    x = sample['signal'].view(1, batch_size, signal_shape)\n",
    "    \n",
    "    # STEP 2\n",
    "#     print(\"x\",x,\"\\n\\n np.shape(x)\",np.shape(x))\n",
    "    z = encoder(x)\n",
    "    valid_z = critic_z(z)\n",
    "    valid_z = torch.squeeze(valid_z)\n",
    "    critic_score_valid_z = torch.mean(torch.ones(valid_z.shape) * valid_z)\n",
    "\n",
    "    # STEP 3\n",
    "    z_ = torch.empty(1, batch_size, latent_space_dim).uniform_(0, 1)\n",
    "    fake_z = critic_z(z_)\n",
    "    fake_z = torch.squeeze(fake_z)\n",
    "    critic_score_fake_z = torch.mean(torch.ones(fake_z.shape) * fake_z) #Wasserstein Loss\n",
    "\n",
    "    # STEP 4\n",
    "    alpha = torch.rand(z.shape)\n",
    "    iz = Variable(alpha * z + (1 - alpha) * z_) #Random Weighted Average\n",
    "    iz.requires_grad_(True)\n",
    "    v_iz = critic_z(iz)\n",
    "    v_iz.mean().backward()\n",
    "    gradients = iz.grad\n",
    "    gp_loss = torch.sqrt(torch.sum(torch.square(gradients).view(-1)))\n",
    "    \n",
    "    # STEP 5\n",
    "    wl = critic_score_fake_z - critic_score_valid_z\n",
    "    loss = wl + gp_loss\n",
    "    \n",
    "    # STEP 6\n",
    "    loss.backward()\n",
    "    \n",
    "    # STEP 7\n",
    "    optim_cz.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-pillow",
   "metadata": {},
   "source": [
    "### Define the Encoder training(compute the adapted cycle consistency loss on encoder)\n",
    "\n",
    "# ADD INFORMATION ON CYCLE CONSISTENCY LOSS AND HOW THE CRITICS OUTPUT ARE USED TO COMPUTE SCORE\n",
    "\n",
    "From a sample a data (*subdivision of the original time series*) we generate $Z={(z_{1},z_{2},...,z_{k})}$ from a random space following normal distribution, where $k$ denotes the dimension of the latent space. Here we **encode $x$ in $z$**.\n",
    "\n",
    "- STEP 1 : Resizing x\n",
    "- STEP 2 : Estimate the Anomaly Scores with Critic X Outputs\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{m} \\sum_{i=1}^m C_{x}(x_{i}) \\\\\n",
    "\\end{equation}\n",
    "\n",
    "- STEP 3 :Estimate the Anomaly Scores with Critic X Outputs\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{m} \\sum_{i=1}^m C_{x}(G(z_{i})) \\\\\n",
    "\\end{equation}\n",
    "\n",
    "- STEP 4 :\n",
    "\\begin{equation}\n",
    "\\frac{1}{m} \\sum_{i=1}^m ||x_{i} - G(E(x_{i}))||^2 \\\\\n",
    "\\end{equation}\n",
    "\n",
    "- STEP 5 :\n",
    "\n",
    "$$step 2 - step 3 + step 4$$\n",
    "\n",
    "- STEP 6 : Compute the the gradient on $w_{G}$ and $w_{E}$\n",
    "- STEP 7 : Update the weights $w_{G,E}$\n",
    "\n",
    "\\begin{equation}\n",
    "w_{G,E} =w_{G,E} + stepSize* adam(w_{G,E}, STEP5)\\\\\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "elegant-conviction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_iteration(sample):\n",
    "    optim_enc.zero_grad()\n",
    "    \n",
    "    # STEP 1 :\n",
    "    x = sample['signal'].view(1, batch_size, signal_shape)\n",
    "    \n",
    "    # STEP 2\n",
    "    valid_x = critic_x(x)\n",
    "    valid_x = torch.squeeze(valid_x)\n",
    "    critic_score_valid_x = torch.mean(torch.ones(valid_x.shape) * valid_x) #Wasserstein Loss\n",
    "    \n",
    "    # STEP3\n",
    "    z = torch.empty(1, batch_size, latent_space_dim).uniform_(0, 1)\n",
    "    x_ = decoder(z)\n",
    "    fake_x = critic_x(x_)\n",
    "    fake_x = torch.squeeze(fake_x)\n",
    "    critic_score_fake_x = torch.mean(torch.ones(fake_x.shape) * fake_x)\n",
    "    \n",
    "    #STEP4\n",
    "    enc_z = encoder(x)\n",
    "    gen_x = decoder(enc_z)\n",
    "\n",
    "    mse = mse_loss(x.float(), gen_x.float())\n",
    "    loss_enc = mse + critic_score_valid_x - critic_score_fake_x\n",
    "    loss_enc.backward(retain_graph=True)\n",
    "    optim_enc.step()\n",
    "\n",
    "    return loss_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-prompt",
   "metadata": {},
   "source": [
    "### Define the Decoder training (compute the adapted cycle consistency loss on decoder)\n",
    "\n",
    "From a sample a data (*subdivision of the original time series*) we generate $Z={(z_{1},z_{2},...,z_{k})}$ from a random space following normal distribution, where $k$ denotes the dimension of the latent space. Here we **encode $x$ in $z$**.\n",
    "\n",
    "- STEP 1 : Resizing x\n",
    "- STEP 2 :\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{m} \\sum_{i=1}^m C_{z}(z_{i}) \\\\\n",
    "\\end{equation}\n",
    "\n",
    "- STEP 3 :\n",
    "\\begin{equation}\n",
    "\\frac{1}{m} \\sum_{i=1}^m C_{z}(E(x_{i})) \\\\\n",
    "\\end{equation}\n",
    "\n",
    "- STEP 4 :\n",
    "\\begin{equation}\n",
    "\\frac{1}{m} \\sum_{i=1}^m ||x_{i} - G(E(x_{i}))||^2 \\\\\n",
    "\\end{equation}\n",
    "\n",
    "- STEP 5 :\n",
    "\n",
    "$$step 2 - step 3 + step 4$$\n",
    "\n",
    "- STEP 6 : Compute the the gradient on $w_{G}$ and $w_{E}$\n",
    "- STEP 7 : Update the weights $w_{G,E}$\n",
    "\n",
    "\\begin{equation}\n",
    "w_{G,E} =w_{G,E} + stepSize* adam(w_{G,E}, STEP5)\\\\\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "developmental-webcam",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_iteration(sample):\n",
    "    optim_dec.zero_grad()# VERY IMPORTANT TO PUT IT HERE\n",
    "    \n",
    "    #STEP 1\n",
    "    x = sample['signal'].view(1, batch_size, signal_shape)\n",
    "    z = encoder(x)\n",
    "    \n",
    "    #STEP 2\n",
    "    #Estimating Anomaly Scores with Critic Z Outputs\n",
    "    valid_z = critic_z(z) # Cz(z)\n",
    "    valid_z = torch.squeeze(valid_z) # resize by making it a univariate variable\n",
    "    critic_score_valid_z = torch.mean(torch.ones(valid_z.shape) * valid_z) #compute the score of criticZ\n",
    "    \n",
    "    #STEP 3\n",
    "    #we use \n",
    "    z_ = torch.empty(1, batch_size, latent_space_dim).uniform_(0, 1)\n",
    "    fake_z = critic_z(z_)\n",
    "    fake_z = torch.squeeze(fake_z)\n",
    "    critic_score_fake_z = torch.mean(torch.ones(fake_z.shape) * fake_z) #\n",
    "    \n",
    "    #STEP 4\n",
    "    enc_z = encoder(x)\n",
    "    gen_x = decoder(enc_z)\n",
    "    mse = mse_loss(x.float(), gen_x.float())\n",
    "    \n",
    "    #STEP 5\n",
    "    loss_dec = mse + critic_score_valid_z - critic_score_fake_z\n",
    "    \n",
    "    #STEP 6\n",
    "    loss_dec.backward(retain_graph=True)\n",
    "    \n",
    "    #STEP 7\n",
    "    optim_dec.step()\n",
    "\n",
    "    return loss_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-nicaragua",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-south",
   "metadata": {},
   "source": [
    "<img src =\"./images/tadgan_algo_annotaded.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "amber-principal",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "n_critics = number of iterations of the critic per\n",
    "n_epoch = number of iterations over the data\n",
    "\n",
    "'''\n",
    "\n",
    "def train(n_epochs=2000):\n",
    "    logging.debug('Starting training')\n",
    "    cx_epoch_loss = list()\n",
    "    cz_epoch_loss = list()\n",
    "    encoder_epoch_loss = list()\n",
    "    decoder_epoch_loss = list()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        logging.debug('Epoch {}'.format(epoch))\n",
    "        n_critics = 5\n",
    "\n",
    "        cx_nc_loss = list()\n",
    "        cz_nc_loss = list()\n",
    "\n",
    "        for i in range(n_critics):\n",
    "            cx_loss = list() \n",
    "            cz_loss = list()\n",
    "            \n",
    "            '''this part corresponds to part 1 of the image above'''\n",
    "            for batch, sample in enumerate(train_loader):\n",
    "                loss = critic_x_iteration(sample) #compute the Wasserstein loss for critic X\n",
    "                cx_loss.append(loss)\n",
    "\n",
    "                loss = critic_z_iteration(sample)#compute the Wasserstein loss for critic Z\n",
    "                cz_loss.append(loss)\n",
    "\n",
    "            cx_nc_loss.append(torch.mean(torch.tensor(cx_loss))) # compute the mean of criticx loss on all samples\n",
    "            cz_nc_loss.append(torch.mean(torch.tensor(cz_loss))) #  compute the mean of criticx loss on all samples\n",
    "\n",
    "        logging.debug('Critic training done in epoch {}'.format(epoch))\n",
    "        encoder_loss = list()\n",
    "        decoder_loss = list()\n",
    "        \n",
    "        '''this part corresponds to part 2 of the image above'''\n",
    "        '''train the generative network E and G with the adapted cycle\n",
    "        consistency loss by minimizing the L2 norm of the difference\n",
    "        between the original and the reconstructed samples'''\n",
    "        for batch, sample in enumerate(train_loader):\n",
    "            enc_loss = encoder_iteration(sample) #compute the adapted cycle consistency loss by minimizing the L2 norm on the encoder\n",
    "            dec_loss = decoder_iteration(sample) #compute the adapted cycle consistency loss by minimizing the L2 norm on the decoder\n",
    "            encoder_loss.append(enc_loss) #add the encoder loss in a tab where the indice corresponds to the sample\n",
    "            decoder_loss.append(dec_loss)\n",
    "        \n",
    "        \n",
    "        # preprocess the data for printing\n",
    "        cx_epoch_loss.append(torch.mean(torch.tensor(cx_nc_loss))) \n",
    "        cz_epoch_loss.append(torch.mean(torch.tensor(cz_nc_loss)))\n",
    "        encoder_epoch_loss.append(torch.mean(torch.tensor(encoder_loss)))\n",
    "        decoder_epoch_loss.append(torch.mean(torch.tensor(decoder_loss)))\n",
    "        \n",
    "        \n",
    "        logging.debug('Encoder decoder training done in epoch {}'.format(epoch))\n",
    "        logging.debug('critic x loss {:.3f} critic z loss {:.3f} \\nencoder loss {:.3f} decoder loss {:.3f}\\n'.format(cx_epoch_loss[-1], cz_epoch_loss[-1], encoder_epoch_loss[-1], decoder_epoch_loss[-1]))\n",
    "        \n",
    "        # save data if the encoder, decoder, critic_x and critic_z every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            torch.save(encoder.state_dict(), encoder.encoder_path)\n",
    "            torch.save(decoder.state_dict(), decoder.decoder_path)\n",
    "            torch.save(critic_x.state_dict(), critic_x.critic_x_path)\n",
    "            torch.save(critic_z.state_dict(), critic_z.critic_z_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-ministry",
   "metadata": {},
   "source": [
    "### Test the model\n",
    "\n",
    "<img src=\"./images/launch_training.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "unknown-solomon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, encoder, decoder, critic_x):\n",
    "    reconstruction_error = list()\n",
    "    critic_score = list()\n",
    "    y_true = list()\n",
    "\n",
    "    for batch, sample in enumerate(test_loader):\n",
    "        reconstructed_signal = decoder(encoder(sample['signal']))\n",
    "        reconstructed_signal = torch.squeeze(reconstructed_signal)\n",
    "\n",
    "        for i in range(0, 64):\n",
    "            x_ = reconstructed_signal[i].detach().numpy()\n",
    "            x = sample['signal'][i].numpy()\n",
    "            y_true.append(int(sample['anomaly'][i].detach()))\n",
    "            reconstruction_error.append(dtw_reconstruction_error(x, x_))\n",
    "        critic_score.extend(torch.squeeze(critic_x(sample['signal'])).detach().numpy())\n",
    "\n",
    "    reconstruction_error = stats.zscore(reconstruction_error)\n",
    "    critic_score = stats.zscore(critic_score)\n",
    "    anomaly_score = reconstruction_error * critic_score\n",
    "    y_predict = detect_anomaly(anomaly_score)\n",
    "    y_predict = prune_false_positive(y_predict, anomaly_score, change_threshold=0.1)\n",
    "    find_scores(y_true, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-ethiopia",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Launch the training</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "right-finland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp 0 \n",
      "\n",
      "tn 384 \n",
      "\n",
      "fp 0 \n",
      "\n",
      "fn 0\n",
      "Accuracy 1.00\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-65d7f1737dd3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;31m#Launch the testing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcritic_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-549b64ea8df9>\u001b[0m in \u001b[0;36mtest\u001b[1;34m(test_loader, encoder, decoder, critic_x)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0my_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetect_anomaly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manomaly_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0my_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprune_false_positive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_predict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manomaly_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchange_threshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mfind_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_predict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-2463590d80ee>\u001b[0m in \u001b[0;36mfind_scores\u001b[1;34m(y_true, y_predict)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tp\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\\n\\ntn\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\\n\\nfp\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\\n\\nfn\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy {:.2f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtp\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mprecision\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtp\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtp\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mrecall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtp\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtp\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Precision {:.2f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This part corresponds to the 3rd part of the above image.\n",
    "'''\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #PREPROCESSING\n",
    "    \n",
    "    dataset = pd.read_csv('exchange-2_cpc_results.csv') # Original file: exchange-2_cpc_results.csv\n",
    "    #Splitting intro train and test\n",
    "    #TODO could be done in a more pythonic way\n",
    "    train_len = int(0.7 * dataset.shape[0])\n",
    "    dataset[0:train_len].to_csv('train_dataset.csv', index=False)\n",
    "    dataset[train_len:].to_csv('test_dataset.csv', index=False)\n",
    "\n",
    "    train_dataset = SignalDataset(path='train_dataset.csv')\n",
    "    test_dataset = SignalDataset(path='test_dataset.csv')\n",
    "    batch_size = 64 #define the batch size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, drop_last=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "    logging.info('Number of train datapoints is {}'.format(len(train_dataset)))\n",
    "    logging.info('Number of samples in train dataset {}'.format(len(train_dataset)))\n",
    "\n",
    "    lr = 1e-6\n",
    "\n",
    "    signal_shape = 100\n",
    "    latent_space_dim = 20\n",
    "    encoder_path = 'models/encoder.pt'\n",
    "    decoder_path = 'models/decoder.pt'\n",
    "    critic_x_path = 'models/critic_x.pt'\n",
    "    critic_z_path = 'models/critic_z.pt'\n",
    "    \n",
    "    encoder = Encoder(encoder_path, signal_shape)\n",
    "    decoder = Decoder(decoder_path, signal_shape)\n",
    "    critic_x = CriticX(critic_x_path, signal_shape)\n",
    "    critic_z = CriticZ(critic_z_path)\n",
    "\n",
    "    mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "    optim_enc = optim.Adam(encoder.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optim_dec = optim.Adam(decoder.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optim_cx = optim.Adam(critic_x.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optim_cz = optim.Adam(critic_z.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "    #launch the training\n",
    "    train(n_epochs=1)\n",
    "    \n",
    "    #Launch the testing\n",
    "    test(test_loader, encoder, decoder, critic_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-postage",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
